# SIIM-FISABIO-RSNA COVID-19 Detection Kaggle Competition
#### Authors : [@theoviel](https://github.com/theoviel)

#### Status :
- WIP


## Introduction

<!-- Our approach is built on understanding the challenges behind the data. Our main contribution is the consideration of the link between healthy glomeruli and unhealthy ones by predicting both into two different classes. We incorporate several external datasets in our pipeline and manually annotated the two classes.
Our model architecture is relatively simple, and the pipeline can be easily transferred to other tasks.

You can read more about our solution [here](https://www.kaggle.com/theoviel/hubmap-final-methodology-submission/). A more concise write-up is also available [here](https://www.kaggle.com/c/hubmap-kidney-segmentation/discussion/238024).

The `main` branch contains a cleaned and simplified version of our pipeline, that is enough to reproduce our solution. -->


## How to use the repository

- Clone the repository
- [TODO : Requirements]

- Download the data :
  - Put the competition data from [Kaggle](https://www.kaggle.com/c/siim-covid19-detection/data) in the `input` folder
  <!-- - You can download pseudo labels [on Kaggle](https://www.kaggle.com/theoviel/hubmap-pl/)
  - We also provide our trained model weights [on Kaggle](https://www.kaggle.com/theoviel/hubmap-cp/) -->

- Prepare the data :
  - Extract the hand labels using `notebooks/Extraction.ipynb` :

- Train models using `notebooks/Training.ipynb`
  - Use the `DEBUG` parameter to launch the code in debug mode (single fold, no logging)
  - Specify the training parameters in the `Config` class. Feel free to experiment with the parameters
  <!-- , here are the main ones : -->
    <!-- - `tile_size` : Tile size
    - `reduce_factor` : Downscaling factor
    - `on_spot_sampling` : Probability to accept a random tile with in the dataset
    - `overlap_factor` : Tile overlapping during inference
    - `selected_folds` : Folds to run computations for.
    - `encoder` : Encoder as defined in [Segmentation Models PyTorch](https://github.com/qubvel/segmentation_models.pytorch)
    - `decoder` : Decoders from [Segmentation Models PyTorch](https://github.com/qubvel/segmentation_models.pytorch)
    - `num_classes` : Number of classes. Keep it at 2 to use the healthy and unhealthy classes
    - `loss` : Loss function. We use the BCE but the lovasz is also interesting
    - `optimizer` : Optimizer name
    - `batch_size` : Training batch size, adapt the `BATCH_SIZES` dictionary to your gpu
    - `val_bs` : Validation batch size
    - `epochs` : Number of training epochs
    - `iter_per_epoch` : Number of tiles to use per epoch
    - `lr` : Learning rate. Will be decayed linearly
    - `warmup_prop` : Proportion of steps to use for learning rate warmup
    - `mix_proba` : Probability to apply MixUp with
    - `mix_alpha` : Alpha parameter for MixUp
    - `use_pl`: Probability to sample a tile from the pseudo-labeled images
    - `use_external`: Probability to sample a tile from the external images
    - `pl_path`: Path to pseudo labels generated by `notebooks/Inference_test.ipynb`
    - `extra_path` : Path to extra labels generated by `notebooks/Json to Mask.ipynb` (should not be changed)
    - `rle_path` : Path to train labels downscaled by `notebooks/Image downscaling.ipynb`  (should not be changed) -->

<!-- - Validate models with `notebooks/Inference.ipynb` :
  - Use the `log_folder` parameter to specify the experiment.
  - Use the `use_tta` parameter to specify whether to use test time augmentations.
  - Use the `save` parameter to indicate whether to save predictions.
  - Use the `save_all_tta` parameter to save predictions for each tta (takes a lot of disk space).
  - Use the `global_threshold` parameter to tweak the threshold.

- Generate pseudo-labels  with `notebooks/Inference Test.ipynb` :
  - Use the `log_folder` parameter to specify the experiment.
  - Use the `use_tta` parameter to speciy whether to use test time augmentations.
  - Use the `save` parameter to indicate whether to save predictions.

- Visualize predictions : `notebooks/Visualize Predictions.ipynb`
  - Works to visualize predictions from the two previous notebooks, but also from a submission file.
  - Specify the `name`, `log_folder` and `sub` parameters according to what you want to plot. 
  -->

<!-- ```
## Code structure (TODO)

If you wish to dive into the code, the repository naming should be straight-forward. Each function is documented.
The structure is the following :


code
├── data
│   ├── dataset.py      # Torch datasets
│   └── transforms.py   # Augmentations
├── inference 
│   ├── main_test.py    #  Inference for the test data
│   └── main.py         # Inference for the train data
├── model_zoo 
│   └── models.py       # Model definition
├── training 
│   ├── lovasz.py       # Lovasz loss implementation
│   ├── main.py         # k-fold and training main functions
│   ├── meter.py        # Meter for evaluation during training
│   ├── mix.py          # CutMix and MixUp
│   ├── optim.py        # Losses and optimizer handling
│   ├── predict.py      # Functions for prediction
│   └── train.py        # Fitting a model
├── utils 
│   ├── logger.py       # Logging utils
│   ├── metrics.py      # Metrics for the competition
│   ├── plots.py        # Plotting utils
│   ├── rle.py          # RLE encoding utils
│   └── torch.py        # Torch utils
└── params.py           # Main parameters
```  -->
