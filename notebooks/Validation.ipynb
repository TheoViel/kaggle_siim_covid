{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to validate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/theo/kaggle/siim_covid/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preparation import prepare_dataframe, handle_duplicates\n",
    "from data.dataset import CovidDetDataset, CovidClsDataset\n",
    "from data.transforms import get_transfos_det, get_transfos_cls\n",
    "\n",
    "from model_zoo.models import get_model\n",
    "\n",
    "from utils.plot import plot_sample\n",
    "\n",
    "from utils.logger import Config\n",
    "\n",
    "from utils.metrics import per_class_average_precision_score, study_level_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\n",
    "    LOG_PATH + \"2021-07-30/4/\",\n",
    "#     LOG_PATH + \"2021-07-31/0/\",\n",
    "    LOG_PATH + \"aphrodeep_v2s_lung/\"\n",
    "]\n",
    "\n",
    "EXP_FOLDER = EXP_FOLDERS[-1]\n",
    "\n",
    "USE_TTA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TTA:\n",
    "    pred_oof_img = np.mean([np.load(f + \"pred_oof_img_flip.npy\") for f in EXP_FOLDERS], 0)\n",
    "    pred_oof_study = np.mean([np.load(f + \"pred_oof_study_flip.npy\") for f in EXP_FOLDERS], 0)\n",
    "else:\n",
    "    pred_oof_img = np.mean([np.load(f + \"pred_oof_img.npy\") for f in EXP_FOLDERS], 0)\n",
    "    pred_oof_study = np.mean([np.load(f + \"pred_oof_study.npy\") for f in EXP_FOLDERS], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(EXP_FOLDER + \"data.csv\")\n",
    "\n",
    "pred_cols = [c + \"_pred\" for c in CLASSES]\n",
    "df[pred_cols] = pred_oof_study\n",
    "df['pred_img'] = pred_oof_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image mAP : 0.962\n",
      "Image AUC : 0.912\n",
      "Image Acc : 0.848\n"
     ]
    }
   ],
   "source": [
    "ap = per_class_average_precision_score(pred_oof_img, df['img_target'].values)\n",
    "print(f'Image mAP : {ap :.3f}')\n",
    "\n",
    "auc = roc_auc_score(df['img_target'], pred_oof_img)\n",
    "print(f'Image AUC : {auc :.3f}')\n",
    "acc = accuracy_score(df['img_target'], pred_oof_img > 0.5)\n",
    "print(f'Image Acc : {acc :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study mAP : 0.3854\n"
     ]
    }
   ],
   "source": [
    "study_map = study_level_map(df[pred_cols].values, df[CLASSES].values, df['study_id'].values)\n",
    "print(f'Study mAP : {study_map :.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Img merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df[['study_id', 'pred_img']].groupby('study_id').mean().rename(\n",
    "    columns={'pred_img': 'pred_img_merged'}\n",
    ").reset_index()\n",
    "df_ = df.merge(groups, on=\"study_id\", how=\"left\")\n",
    "\n",
    "df_.loc[df_['negative_pred'] > 0.75, 'pred_img'] = 0\n",
    "df_.loc[df_['typical_pred'] > 0.75, 'pred_img'] = 1\n",
    "df_.loc[df_['indeterminate_pred'] > 0.75, 'pred_img'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image AUC : 0.906\n",
      "Image Acc : 0.8483\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(df_['img_target'], df_['pred_img'])\n",
    "print(f'Image AUC : {auc :.3f}')\n",
    "acc = accuracy_score(df_['img_target'], df_['pred_img'] > 0.5)\n",
    "print(f'Image Acc : {acc :.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study using img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study mAP : 0.38570\n"
     ]
    }
   ],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "p = 0.4\n",
    "df_['negative_pred'] *= (1 - df_['pred_img']) ** p\n",
    "df_['typical_pred'] *= (df_['pred_img']) ** p\n",
    "\n",
    "# df_['indeterminate_pred'] *= (df_['pred_img']) ** p\n",
    "# df_['atypical_pred'] *= (df_['pred_img']) ** p\n",
    "\n",
    "study_map = study_level_map(df_[pred_cols].values, df_[CLASSES].values, df['study_id'].values)\n",
    "print(f'Study mAP : {study_map :.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.8075, 0.8504, 0.3096, 0.3444]), 0.3853)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_study = df[\n",
    "    ['study_id'] + pred_cols + CLASSES + ['img_target', 'pred_img']\n",
    "].groupby('study_id').agg(np.mean).copy()\n",
    "\n",
    "# df_study['negative_pred'] *= 1 - df_study['pred_img'] \n",
    "# df_study['typical_pred'] *= df_study['pred_img'] \n",
    "# df_study['indeterminate_pred'] *= df_study['pred_img'] \n",
    "# df_study['atypical_pred'] *= df_study['pred_img'] \n",
    "\n",
    "df_study.loc[df_study['pred_img'] > 0.75, 'negative_pred'] *= 0.5\n",
    "df_study.loc[df_study['pred_img'] < 0.2, 'negative_pred'] *= 2\n",
    "\n",
    "df_study.loc[df_study['pred_img'] > 0.75, 'typical_pred'] *= 1.2\n",
    "# df_study.loc[df_study['pred_img'] > 0.9, 'indeterminate_pred'] *= 1.1\n",
    "# df_study.loc[df_study['pred_img'] > 0.9, 'atypical_pred'] *= 1.1\n",
    "\n",
    "df_study.loc[df_study['pred_img'] < 0.25, 'typical_pred'] *= 0.8\n",
    "# df_study.loc[df_study['pred_img'] < 0.2, 'indeterminate_pred'] *= 0.9\n",
    "# df_study.loc[df_study['pred_img'] < 0.2, 'atypical_pred'] *= 0.9\n",
    "\n",
    "accs = per_class_average_precision_score(\n",
    "    df_study[pred_cols].values,\n",
    "    df_study[CLASSES].values, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    average=False\n",
    ")\n",
    "np.round(accs, 4), np.round(np.mean(accs) * 2/3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(x):\n",
    "    x = x.split('0 0 1 1')[:4]\n",
    "    x = [float(y.strip().split(' ')[1]) for y in x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_oof(pred_oof_study):\n",
    "    pred_oof_study['id'] = pred_oof_study['id'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    df_study = pd.read_csv(DATA_PATH + \"train_study_level.csv\")\n",
    "    df_study['study_id'] = df_study['id'].apply(lambda x: x.split('_')[0])\n",
    "    df_study = df_study.rename(columns={c: c.split(' ')[0].lower() for c in df_study.columns})\n",
    "\n",
    "    df_study.drop('id', axis=1, inplace=True)\n",
    "\n",
    "    df_study = df_study.merge(pred_oof_study, how=\"left\", left_on=\"study_id\", right_on=\"id\").dropna()\n",
    "    \n",
    "    pred_oof_study = np.array(df_study['PredictionString'].apply(proc).values.tolist())\n",
    "    \n",
    "    df_study[pred_cols] = pred_oof_study\n",
    "    \n",
    "    df_g = df[['study_id'] + pred_cols].groupby('study_id').mean().reset_index()\n",
    "    df_study = df_study.merge(df_g, how=\"left\", left_on=\"id\", right_on=\"study_id\", suffixes=['', '_theo'])\n",
    "    \n",
    "    return df_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = treat_oof(pd.read_csv('../output/OOF_study_only_EBV2M_768.csv'))\n",
    "df_v2m_2 = treat_oof(pd.read_csv('../output/oof_v2m.csv'))\n",
    "df_b4 = treat_oof(pd.read_csv('../output/oof_b4.csv'))\n",
    "df_b5 = treat_oof(pd.read_csv('../output/oof_b5.csv'))\n",
    "df_ono = treat_oof(pd.read_csv('../output/oof_ono.csv')[['id', 'PredictionString']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_theo = df[['study_id'] + pred_cols + CLASSES].groupby('study_id').mean().reset_index()\n",
    "df_theo = df_v2m_2[['id']].merge(df_theo, how=\"left\", left_on=\"id\", right_on=\"study_id\")\n",
    "\n",
    "df_ono = df_v2m_2[['id']].merge(df_ono, how=\"left\", left_on=\"id\", right_on=\"study_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "oofs = [\n",
    "#     df_old[pred_cols].values ** p * 1,\n",
    "    df_theo[pred_cols].values ** p * 1,\n",
    "    df_v2m_2[pred_cols].values ** p * 1,\n",
    "#     df_b4[pred_cols].values ** p * 0.1,\n",
    "    df_b5[pred_cols].values ** p * 0.5,\n",
    "#     df_ono[pred_cols].values ** p * 0.1,\n",
    "]\n",
    "\n",
    "oof = np.mean(oofs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.392614367480153"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_class_average_precision_score(oof, df_theo[CLASSES].values, num_classes=NUM_CLASSES) * 2 / 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
